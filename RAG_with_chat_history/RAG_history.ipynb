{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with chat history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<img src=\"/home/mauricio/Documents/Projects/RAG-Mastery/Diagrams/RAG-chat-history.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "We will copy part of the code we previously created in the Simple_rag folder. More specifically the load, split and retrieve documents. The part that will be diferent is that the chain is going to be a history retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import tqdm as notebook_tqdm\n",
    "from typing import List, Union\n",
    "\n",
    "# Path to the directory containing config.py\n",
    "config_path = '/home/mauricio/Documents/Projects/RAG-Mastery'\n",
    "\n",
    "# Add the directory to sys.path\n",
    "if config_path not in sys.path:\n",
    "    sys.path.append(config_path)\n",
    "\n",
    "# Now you can import the API_KEY from config.py\n",
    "from config import API_KEY\n",
    "\n",
    "path_to_docs = \"/home/mauricio/Documents/Projects/RAG-Mastery/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "def get_llm_model(self):\n",
    "        return ChatMistralAI(\n",
    "            model_name=\"open-mixtral-8x22b\", \n",
    "            mistral_api_key=self.API_KEY\n",
    "        )\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "def load_documents(folder_path):\n",
    "    documents = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith('.docx') or file.endswith('.pdf') or file.endswith('.txt'):\n",
    "            loader = UnstructuredLoader(os.path.join(folder_path, file))\n",
    "            documents.extend(loader.load())\n",
    "        print(\"Document loaded lenght: \", len(documents))\n",
    "    print(\"Documents loaded successfully ✅\")\n",
    "    print(documents[0].metadata.get(\"filename\"))\n",
    "    return documents\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def split_documents(documents, chunk_size= 1000, chunk_overlap= 200):\n",
    "        try:\n",
    "            text_splitter: RecursiveCharacterTextSplitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                length_function=len,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "            )\n",
    "            splits: List[Document] = text_splitter.split_documents(documents)\n",
    "            print(\"Split document successfully ✅\")\n",
    "            print(\"Documents split: \", len(splits))\n",
    "            return splits\n",
    "        except Exception as e:\n",
    "            print(f\"Error splitting documents: {e}\")\n",
    "            raise\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "def embed_documents(splits):\n",
    "    try:\n",
    "        embeddings = MistralAIEmbeddings(\n",
    "            model=\"mistral-embed\",\n",
    "            mistral_api_key=API_KEY\n",
    "            )\n",
    "        vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\"k\": 6},\n",
    "        )\n",
    "\n",
    "        print(\"Retriever succesfuly created ✅\")\n",
    "        return retriever\n",
    "    except Exception as e:\n",
    "        print(f\"Error embeding/retrieving documents {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's beggin the construction of the history chat. We will stat by adding a sub-chain that takes the lastest user question and reformulates it in the context of the chat history, basically qe want to do this\n",
    "\n",
    "(query, conversation history) -> LLM -> rephrased query -> retriever\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_rag_mastery",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
