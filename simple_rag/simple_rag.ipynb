{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/home/mauricio/Documents/Projects/RAG-Mastery/Diagrams/indexing_retrieval_generation.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import tqdm as notebook_tqdm\n",
    "from typing import List, Union\n",
    "\n",
    "# Path to the directory containing config.py\n",
    "config_path = '/home/mauricio/Documents/Projects/RAG-Mastery'\n",
    "\n",
    "# Add the directory to sys.path\n",
    "if config_path not in sys.path:\n",
    "    sys.path.append(config_path)\n",
    "\n",
    "# Now you can import the API_KEY from config.py\n",
    "from config import API_KEY\n",
    "\n",
    "path_to_docs = \"/home/mauricio/Documents/Projects/RAG-Mastery/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the ChatMistralAI class from langchain. With this, we'll be able to use the AI model from Mistral. In our case, the 7x8b model will be enough for our RAG system. We initialize the model with our API key to access Mistral's services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "def get_llm_model(self):\n",
    "        return ChatMistralAI(\n",
    "            model_name=\"open-mixtral-8x22b\", \n",
    "            mistral_api_key=self.API_KEY\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part, we use an UnstructuredLoader due to the multiple advantages it provides, and later we plan to increase the capabilities of our RAG system with this. The main advantages are that it can handle multiple file formats (.docx, .pdf, .txt) with a single loader and it preserves the document structure. This provides better retention of the original document layout and structure, which can lead to more accurate and context-aware text extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "def load_documents(folder_path):\n",
    "    documents = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith('.docx') or file.endswith('.pdf') or file.endswith('.txt'):\n",
    "            loader = UnstructuredLoader(os.path.join(folder_path, file))\n",
    "            documents.extend(loader.load())\n",
    "        print(\"Document loaded lenght: \", len(documents))\n",
    "    print(\"Documents loaded successfully ✅\")\n",
    "    print(documents[0].metadata.get(\"filename\"))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RecursiveCharacterTextSplitter is ideal for documents with multiple images and text because it adaptively splits content using various separators, preserves context through chunk overlap, and maintains the logical flow of diverse document elements. This approach ensures that the relationship between text and image descriptions is retained, while creating manageable chunks for processing. The flexibility in chunk size and overlap allows for fine-tuning to balance context preservation with efficient processing, making it particularly effective for complex documents in a RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def split_documents(documents, chunk_size= 1000, chunk_overlap= 200):\n",
    "        try:\n",
    "            text_splitter: RecursiveCharacterTextSplitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                length_function=len,\n",
    "                separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "            )\n",
    "            splits: List[Document] = text_splitter.split_documents(documents)\n",
    "            print(\"Split document successfully ✅\")\n",
    "            print(\"Documents split: \", len(splits))\n",
    "            return splits\n",
    "        except Exception as e:\n",
    "            print(f\"Error splitting documents: {e}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a embeddings function that uses the previous documents and splits and the create a retriever for efficient document search and retrieval. \n",
    "\n",
    "The MistralAIEmbeddings is the class provided by langchain that generate the embeddings using the MistralAI API,\n",
    "\n",
    "Then we import the FAISS is a library for efficient similarity seach and clustering of dense vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "def embed_documents(splits):\n",
    "    try:\n",
    "        embeddings = MistralAIEmbeddings(\n",
    "            model=\"mistral-embed\",\n",
    "            mistral_api_key=API_KEY\n",
    "            )\n",
    "        vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\"k\": 6},\n",
    "        )\n",
    "\n",
    "        print(\"Retriever succesfuly created ✅\")\n",
    "        return retriever\n",
    "    except Exception as e:\n",
    "        print(f\"Error embeding/retrieving documents {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finaly we will create a class that performs the RAG by using the retriever we created. This will use the retriever to fetch relevant documents based on a user query. Formatting the retrieved documents and the user query into a structured prompr template.\n",
    "\n",
    "The last step is to invoke the MistralAI lenguage model to generate a response based on the formatted input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "class SimpleRAG:\n",
    "    def __init__(self, retriever, api_key):\n",
    "        self.retriever = retriever\n",
    "        self.API_KEY = api_key\n",
    "        self.llm = get_llm_model(self)\n",
    "        self.prompt_template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are an rule assistant for the NFL. Answer the questions based on the context provided. If the answer to the question is not in the context, say \\\"I don't know\\\".\"),\n",
    "            (\"human\", \"Context: {context}\\n\\nQuestion: {question}\\nAnswer:\")\n",
    "        ])\n",
    "\n",
    "    def rag_chain(self, question):\n",
    "        # Retrieve the relevant documents\n",
    "        context = self.retriever.get_relevant_documents(question)\n",
    "        context_str = \"\\n\".join([doc.page_content for doc in context])\n",
    "        \n",
    "        # Format the messages\n",
    "        messages = self.prompt_template.format_messages(\n",
    "            context=context_str,\n",
    "            question=question\n",
    "        )\n",
    "        \n",
    "        # Generate the answer\n",
    "        response = self.llm.invoke(messages)\n",
    "        \n",
    "        return response\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document loaded lenght:  1964\n",
      "Documents loaded successfully ✅\n",
      "2024-nfl-rulebook.pdf\n",
      "Split document successfully ✅\n",
      "Documents split:  1995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever succesfuly created ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"A running play in American football is a type of offensive play where the offense attempts to advance the ball without a forward pass. The play begins in situations such as when a player hands off the ball to a teammate, or when a player takes the ball from the center and runs with it. The context also mentions that a running play begins when there's obvious video evidence that the runner was not out of bounds. However, please note that this is a simplified explanation and the actual rules regarding running plays can be more complex and detailed.\" response_metadata={'token_usage': {'prompt_tokens': 147, 'total_tokens': 257, 'completion_tokens': 110}, 'model': 'open-mixtral-8x22b', 'finish_reason': 'stop'} id='run-f8f1feda-9a6e-414d-850a-fda5adcef5bf-0' usage_metadata={'input_tokens': 147, 'output_tokens': 110, 'total_tokens': 257}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Suppress the specific warning\n",
    "warnings.filterwarnings(\"ignore\", message=\"Could not download mistral tokenizer\")\n",
    "\n",
    "\n",
    "# Usage\n",
    "documents = load_documents(path_to_docs)\n",
    "splits = split_documents(documents)\n",
    "retriever = embed_documents(splits)\n",
    "\n",
    "api_key = \"your_mistral_api_key\"  # Make sure to replace this with your actual API key\n",
    "rag = SimpleRAG(retriever, API_KEY)\n",
    "\n",
    "query = \"Explain to me the RUNNING PLAY?\"\n",
    "response = rag.rag_chain(query)\n",
    "print(response)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A running play in American football is a type of offensive play where the offense attempts to advance the ball without a forward pass. The play begins in situations such as when a player hands off the ball to a teammate, or when a player takes the ball from the center and runs with it. The context also mentions that a running play begins when there's obvious video evidence that the runner was not out of bounds. However, please note that this is a simplified explanation and the actual rules regarding running plays can be more complex and detailed.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"A running play in American football is a type of offensive play where the offense attempts to advance the ball without a forward pass. The play begins in situations such as when a player hands off the ball to a teammate, or when a player takes the ball from the center and runs with it. The context also mentions that a running play begins when there's obvious video evidence that the runner was not out of bounds. However, please note that this is a simplified explanation and the actual rules regarding running plays can be more complex and detailed.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_rag_mastery",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
